神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为**激活函数**（又称激励函数）。

如果不用激励函数（其实相当于激励函数是$f(x) = x$），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。

早期研究神经网络主要采用sigmoid函数或者tanh函数，输出有界，很容易充当下一层的输入。近些年Relu函数及其改进型（如Leaky-ReLU、P-ReLU、R-ReLU等）在多层神经网络中应用比较多。

## Sigmoid
$$
f(z) = \frac{1}{1+e^{-z}}
$$
特点：
* 它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
  缺点：
* 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。如果我们初始化神经网络的权值为 $[0,1][0,1][0,1]$ 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 $(1,+∞)(1,+∞)(1,+∞)$ 区间内的值，则会出现梯度爆炸情况。
* Sigmoid 的 output 不是0均值（即**zero-centered**）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如$x>0, f = w^Tx+bx>0$，那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
* 其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。

## tanh
$$
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

tanh读作Hyperbolic Tangent，它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。

## Relu函数
```latex
Relu(x) = max(0, x)
```

ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
* 解决了gradient vanishing问题 (在正区间)
* 计算速度非常快，只需要判断输入是否大于0
* 收敛速度远快于sigmoid和tanh
  **ReLU目前仍是最常用的activation function**

## Leaky ReLU (PReLU)
$$
f(x) = max(\alpha x, x), \alpha = 0.01
$$

为了解决Dead ReLU Problem

## ELU (Exponential Linear Units)
$$
f(x) = x, if x>0;
f(x) = \alpha(e^x - 1), otherwise
$$

特点：
* 不会有Dead ReLU问题
* 输出的均值接近0，zero-centered